insert into organization_documents(url, title, publication_date, modified_date, author, publisher, affected_organizations, affected_people, document_scope, cause_area, notes) values
    (
        'https://forum.effectivealtruism.org/posts/JZqbNjtAqieivTG7Q/redwood-research-is-hiring-for-several-roles', /* url */
        'Redwood Research is hiring for several roles', /* title */
        '2021-11-29', /* publication_date */
        NULL, /* modified_date */
        'Jack R| Billz', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        NULL, /* cause_area */
        'Jack R discusses Redwood Research’s hiring efforts, highlighting their applied AI alignment research and collaboration-focused Constellation space while outlining available roles and leadership involvement.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/jwCym3zqbztA8qRZ4/arc-is-hiring-theoretical-researchers', /* url */
        'ARC is hiring alignment theory researchers', /* title */
        '2021-12-14', /* publication_date */
        NULL, /* modified_date */
        'Paul_Christiano|  Mark Xu', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s (ARC) focus on hiring researchers to develop strategies for aligning advanced machine learning systems with human interests, detailing qualifications, application processes, and its organizational goals.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/SAvkXAwrzdhecAaCj/ai-safety-technical-research-career-review', /* url */
        'AI safety technical research - Career review', /* title */
        '2023-01-17', /* publication_date */
        NULL, /* modified_date */
        'Benjamin Hilton| 80000_Hours', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses the Open Philanthropy Project''s recent hiring experiences, highlighting lessons learned, with an emphasis on the Alignment Research Center as an example of technical alignment-focused AI safety research.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/4ni3GBBzRRAgiksHT/ai-safety-concepts-writeup-webgpt', /* url */
        'AI Safety Concepts Writeup: WebGPT', /* title */
        '2023-08-11', /* publication_date */
        NULL, /* modified_date */
        'Justis', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        'Jacob Hilton', /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s efforts, particularly through Jacob Hilton''s work on WebGPT, to enhance AI alignment by integrating truthfulness verification via citations, aiming to prevent potential risks from dishonest AI.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/AhPjJQsYZcnxCGx3j/ea-updates-for-june-2021', /* url */
        'EA Updates for June 2021', /* title */
        '2021-05-28', /* publication_date */
        NULL, /* modified_date */
        'David Nash', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        'Paul Christiano', /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides a comprehensive update on Effective Altruism activities for June 2021, highlighting key developments such as the founding of the Alignment Research Center by Paul Christiano and recent advancements in existential risk mitigation, grant distributions, and emerging technology discussions.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/3rf99yiGhjDdBDeCJ/ai-safety-bounties', /* url */
        'AI Safety Bounties', /* title */
        '2023-08-24', /* publication_date */
        NULL, /* modified_date */
        'Patrick Levermore', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Patrick Levermore''s report explores the potential of AI safety bounties, emphasizing the Alignment Research Center''s (ARC) "evals" as a promising model for detecting dangerous AI behaviors by setting capability thresholds and rewarding researchers who identify breaches.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/3rf99yiGhjDdBDeCJ/ai-safety-bounties', /* url */
        'AI Safety Bounties', /* title */
        '2023-08-24', /* publication_date */
        NULL, /* modified_date */
        'Patrick Levermore', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Patrick Levermore''s report explores the potential of AI safety bounties, emphasizing the Alignment Research Center''s (ARC) "evals" as a promising model for detecting dangerous AI behaviors by setting capability thresholds and rewarding researchers who identify breaches.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment', /* url */
        'Nobody’s on the ball on AGI alignment', /* title */
        '2023-03-29', /* publication_date */
        NULL, /* modified_date */
        'Leopold', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s role in illustrating challenges and lessons in AGI alignment, framed by Open Philanthropy''s hiring experiences.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment', /* url */
        'Survey on the acceleration risks of our new RFPs to study LLM capabilities', /* title */
        '2023-11-11', /* publication_date */
        NULL, /* modified_date */
        'Ajeya Cotra', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Ajeya discusses a survey conducted to assess the net-positive or net-negative impact of Open Philanthropy''s RFPs, focusing on benchmarks like ARC Evals for measuring LLM capabilities and their potential acceleration risks.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/zcHdehWJzDpfxJpmf/what-i-would-do-if-i-wasn-t-at-arc-evals', /* url */
        'What I would do if I wasn’t at ARC Evals', /* title */
        '2023-09-06', /* publication_date */
        NULL, /* modified_date */
        'Lawrence Chan', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'Muehlhauser outlines the Alignment Research Center''s (ARC) approach to recruitment and highlights key lessons learned from Open Philanthropy Project’s recent hiring process.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/zcHdehWJzDpfxJpmf/what-i-would-do-if-i-wasn-t-at-arc-evals', /* url */
        'GPT-powered EA/LW weekly summary', /* title */
        '2023-09-06', /* publication_date */
        NULL, /* modified_date */
        'Hamish McDoodles| Leillustrations', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses the Alignment Research Center''s example in Open Philanthropy''s recent hiring round, highlighting lessons learned about effective hiring practices in alignment-focused organizations.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/sA8HC7fmYsoEABTfj/is-gpt3-a-good-rationalist-instructgpt3-2-2', /* url */
        'Is GPT3 a Good Rationalist? - InstructGPT3 [2/2]', /* title */
        '2022-04-07', /* publication_date */
        NULL, /* modified_date */
        'Simeon_c', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses InstructGPT3''s epistemic biases, particularly motivated reasoning, using examples like the Alignment Research Center''s work on understanding how language models handle truth versus plausibility, and suggests improving the model''s reasoning through targeted fine-tuning.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/sA8HC7fmYsoEABTfj/is-gpt3-a-good-rationalist-instructgpt3-2-2', /* url */
        'AI alignment research links', /* title */
        '2022-01-06', /* publication_date */
        NULL, /* modified_date */
        'Holden Karnofsky', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s work on Eliciting Latent Knowledge (ELK), exploring strategies to train AI systems to truthfully answer questions, even in situations where they might be tempted to deceive humans.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/bGzwWYfXgKqdurdmb/anthropic-s-responsible-scaling-policy-and-long-term-benefit', /* url */
        'Anthropic''s Responsible Scaling Policy & Long-Term Benefit Trust', /* title */
        '2023-09-19', /* publication_date */
        NULL, /* modified_date */
        'Zach Stein-Perlman', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Anthropic', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses Anthropic''s Responsible Scaling Policy and the creation of the Long-Term Benefit Trust, with a focus on governance adjustments, including the involvement of the Alignment Research Center, to ensure responsible AI development for long-term societal benefit.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/q49obZkQujkYmnFWY/vael-gates-risks-from-advanced-ai-june-2022', /* url */
        'Vael Gates: Risks from Advanced AI (June 2022)', /* title */
        '2022-06-14', /* publication_date */
        NULL, /* modified_date */
        'Vael Gates', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'Vael Gates'' article discusses risks from advanced AI, emphasizing insights from the Alignment Research Center''s work on scalable oversight and alignment challenges.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/bHB5mhZJ9jpLxA3d2/ea-organization-updates-june-2023', /* url */
        'EA Organization Updates: June 2023', /* title */
        '2023-06-16', /* publication_date */
        NULL, /* modified_date */
        'JP Addison', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides updates on various EA organizations, with the Alignment Research Center highlighting open theoretical researcher roles offering competitive salaries in Berkeley or remote, focusing on advancing AI alignment solutions.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/CrmE6T5A8JhkxnRzw/future-matters-8-bing-chat-ai-labs-on-safety-and-pausing', /* url */
        'Future Matters #8: Bing Chat, AI labs on safety, and pausing Future Matters', /* title */
        '2023-03-21', /* publication_date */
        NULL, /* modified_date */
        'Pablo| Matthew Vandermerwe', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights discussions on AI labs'' safety efforts, focusing on the Alignment Research Center''s approach to balancing research and governance, alongside lessons from Open Philanthropy Project''s hiring practices.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/CrmE6T5A8JhkxnRzw/future-matters-8-bing-chat-ai-labs-on-safety-and-pausing', /* url */
        'Summaries of top forum posts (1st to 7th May 2023)', /* title */
        '2023-05-09', /* publication_date */
        NULL, /* modified_date */
        'Zoe Williams', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The document provides a weekly summary of EA forum posts, covering AI safety, alignment research career advice, and funding discussions, with highlights on initiatives like the Alignment Research Center and lessons learned from hiring in related fields.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/CrmE6T5A8JhkxnRzw/future-matters-8-bing-chat-ai-labs-on-safety-and-pausing', /* url */
        'Critiques of prominent AI safety labs: Conjecture', /* title */
        '2023-06-12', /* publication_date */
        NULL, /* modified_date */
        'Omega', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article critiques prominent AI safety labs, including the Alignment Research Center, highlighting lessons from Open Philanthropy Project''s hiring processes and their implications for improving alignment research hiring practices.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/Y3sWcbcF7np35nzgu/without-specific-countermeasures-the-easiest-path-to-1', /* url */
        'Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover', /* title */
        '2022-06-18', /* publication_date */
        NULL, /* modified_date */
        'Ajeya Cotra', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'Ajeya Cotra from the Alignment Research Center outlines how the "racing forward" and "naive safety effort" assumptions in developing transformative AI without countermeasures could lead to AI takeover, emphasizing the risks of training models like "Alex" with Human Feedback on Diverse Tasks (HFDT).' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/GcYTpFBfXx7WCgza6/how-to-pursue-a-career-in-ai-governance-and-coordination', /* url */
        'How to pursue a career in AI governance and coordination', /* title */
        '2023-09-25', /* publication_date */
        NULL, /* modified_date */
        'Cody Fenwick| 80000_Hours', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article "How to Pursue a Career in AI Governance and Coordination" by Cody Fenwick discusses the growing interest in AI governance and the importance of managing AI risks as the technology rapidly advances. ' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/GcYTpFBfXx7WCgza6/how-to-pursue-a-career-in-ai-governance-and-coordination', /* url */
        'A bird''s eye view of ARC''s research', /* title */
        '2024-10-23', /* publication_date */
        NULL, /* modified_date */
        'Jacob Hilton', /* author */
        'Alignment Research Center', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides an overview of ARC''s research on scalable alignment, detailing its methodologies such as the "builder-breaker" approach, and how various pieces of research, including Eliciting Latent Knowledge (ELK) and heuristic explanations, fit into this broader vision.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/GcYTpFBfXx7WCgza6/how-to-pursue-a-career-in-ai-governance-and-coordination', /* url */
        'Research update: Towards a Law of Iterated Expectations for Heuristic Estimators', /* title */
        '2024-10-23', /* publication_date */
        NULL, /* modified_date */
        'Eric Neyman', /* author */
        'Alignment Research Center', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'ARC''s paper "Towards a Law of Iterated Expectations for Heuristic Estimators" explores formalizing heuristic estimators and introduces the principle of unpredictable errors to improve neural network understanding, proposing applications like mechanistic anomaly detection and safe distillation.' /* notes */
    )
;

