insert into organization_documents(url, title, publication_date, modified_date, author, publisher, affected_organizations, affected_people, document_scope, cause_area, notes) values
    (
        'https://forum.effectivealtruism.org/posts/JZqbNjtAqieivTG7Q/redwood-research-is-hiring-for-several-roles', /* url */
        'Redwood Research is hiring for several roles', /* title */
        '2021-11-29', /* publication_date */
        NULL, /* modified_date */
        'Jack R| Billz', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        NULL, /* cause_area */
        'Jack R discusses Redwood Research’s hiring efforts, highlighting their applied AI alignment research and collaboration-focused Constellation space while outlining available roles and leadership involvement.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/jwCym3zqbztA8qRZ4/arc-is-hiring-theoretical-researchers', /* url */
        'ARC is hiring alignment theory researchers', /* title */
        '2021-12-14', /* publication_date */
        NULL, /* modified_date */
        'Paul_Christiano|  Mark Xu', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s (ARC) focus on hiring researchers to develop strategies for aligning advanced machine learning systems with human interests, detailing qualifications, application processes, and its organizational goals.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/SAvkXAwrzdhecAaCj/ai-safety-technical-research-career-review', /* url */
        'AI safety technical research - Career review', /* title */
        '2023-01-17', /* publication_date */
        NULL, /* modified_date */
        'Benjamin Hilton| 80000_Hours', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses the Open Philanthropy Project''s recent hiring experiences, highlighting lessons learned, with an emphasis on the Alignment Research Center as an example of technical alignment-focused AI safety research.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/4ni3GBBzRRAgiksHT/ai-safety-concepts-writeup-webgpt', /* url */
        'AI Safety Concepts Writeup: WebGPT', /* title */
        '2023-08-11', /* publication_date */
        NULL, /* modified_date */
        'Justis', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        'Jacob Hilton', /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s efforts, particularly through Jacob Hilton''s work on WebGPT, to enhance AI alignment by integrating truthfulness verification via citations, aiming to prevent potential risks from dishonest AI.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/AhPjJQsYZcnxCGx3j/ea-updates-for-june-2021', /* url */
        'EA Updates for June 2021', /* title */
        '2021-05-28', /* publication_date */
        NULL, /* modified_date */
        'David Nash', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        'Paul Christiano', /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides a comprehensive update on Effective Altruism activities for June 2021, highlighting key developments such as the founding of the Alignment Research Center by Paul Christiano and recent advancements in existential risk mitigation, grant distributions, and emerging technology discussions.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/3rf99yiGhjDdBDeCJ/ai-safety-bounties', /* url */
        'AI Safety Bounties', /* title */
        '2023-08-24', /* publication_date */
        NULL, /* modified_date */
        'Patrick Levermore', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Open Philanthropy', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Patrick Levermore''s report explores the potential of AI safety bounties, emphasizing the Alignment Research Center''s (ARC) "evals" as a promising model for detecting dangerous AI behaviors by setting capability thresholds and rewarding researchers who identify breaches.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/3rf99yiGhjDdBDeCJ/ai-safety-bounties', /* url */
        'AI Safety Bounties', /* title */
        '2023-08-24', /* publication_date */
        NULL, /* modified_date */
        'Patrick Levermore', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Patrick Levermore''s report explores the potential of AI safety bounties, emphasizing the Alignment Research Center''s (ARC) "evals" as a promising model for detecting dangerous AI behaviors by setting capability thresholds and rewarding researchers who identify breaches.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment', /* url */
        'Nobody’s on the ball on AGI alignment', /* title */
        '2023-03-29', /* publication_date */
        NULL, /* modified_date */
        'Leopold', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s role in illustrating challenges and lessons in AGI alignment, framed by Open Philanthropy''s hiring experiences.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/5LNxeWFdoynvgZeik/nobody-s-on-the-ball-on-agi-alignment', /* url */
        'Survey on the acceleration risks of our new RFPs to study LLM capabilities', /* title */
        '2023-11-11', /* publication_date */
        NULL, /* modified_date */
        'Ajeya Cotra', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Ajeya discusses a survey conducted to assess the net-positive or net-negative impact of Open Philanthropy''s RFPs, focusing on benchmarks like ARC Evals for measuring LLM capabilities and their potential acceleration risks.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/zcHdehWJzDpfxJpmf/what-i-would-do-if-i-wasn-t-at-arc-evals', /* url */
        'What I would do if I wasn’t at ARC Evals', /* title */
        '2023-09-06', /* publication_date */
        NULL, /* modified_date */
        'Lawrence Chan', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'Muehlhauser outlines the Alignment Research Center''s (ARC) approach to recruitment and highlights key lessons learned from Open Philanthropy Project’s recent hiring process.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/zcHdehWJzDpfxJpmf/what-i-would-do-if-i-wasn-t-at-arc-evals', /* url */
        'GPT-powered EA/LW weekly summary', /* title */
        '2023-09-06', /* publication_date */
        NULL, /* modified_date */
        'Hamish McDoodles| Leillustrations', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses the Alignment Research Center''s example in Open Philanthropy''s recent hiring round, highlighting lessons learned about effective hiring practices in alignment-focused organizations.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/sA8HC7fmYsoEABTfj/is-gpt3-a-good-rationalist-instructgpt3-2-2', /* url */
        'Is GPT3 a Good Rationalist? - InstructGPT3 [2/2]', /* title */
        '2022-04-07', /* publication_date */
        NULL, /* modified_date */
        'Simeon_c', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses InstructGPT3''s epistemic biases, particularly motivated reasoning, using examples like the Alignment Research Center''s work on understanding how language models handle truth versus plausibility, and suggests improving the model''s reasoning through targeted fine-tuning.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/sA8HC7fmYsoEABTfj/is-gpt3-a-good-rationalist-instructgpt3-2-2', /* url */
        'AI alignment research links', /* title */
        '2022-01-06', /* publication_date */
        NULL, /* modified_date */
        'Holden Karnofsky', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article highlights the Alignment Research Center''s work on Eliciting Latent Knowledge (ELK), exploring strategies to train AI systems to truthfully answer questions, even in situations where they might be tempted to deceive humans.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/bGzwWYfXgKqdurdmb/anthropic-s-responsible-scaling-policy-and-long-term-benefit', /* url */
        'Anthropic''s Responsible Scaling Policy & Long-Term Benefit Trust', /* title */
        '2023-09-19', /* publication_date */
        NULL, /* modified_date */
        'Zach Stein-Perlman', /* author */
        'Effective Altruism Forum', /* publisher */
        'Alignment Research Center| Anthropic', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses Anthropic''s Responsible Scaling Policy and the creation of the Long-Term Benefit Trust, with a focus on governance adjustments, including the involvement of the Alignment Research Center, to ensure responsible AI development for long-term societal benefit.' /* notes */
    )
;

