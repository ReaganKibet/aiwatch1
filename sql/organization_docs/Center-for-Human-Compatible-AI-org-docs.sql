insert into organization_documents(url, title, publication_date, modified_date, author, publisher, affected_organizations, affected_people, document_scope, cause_area, notes) values
    (
        'https://forum.effectivealtruism.org/posts/59egqFgZBrfPqXWTr/ama-we-work-in-operations-at-ea-aligned-organizations-ask-us', /* url */
        'AMA: We Work in Operations at EA-aligned organizations. Ask Us Anything.', /* title */
        '2021-02-03', /* publication_date */
        NULL, /* modified_date */
        'Marisa', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'AMA', /* document_scope */
        'Effective altruism' /* cause_area */
        'This article offers an overview of operations work within EA-aligned organizations, highlighting roles like those at the Center for Human-Compatible AI, where staff manage tasks ranging from hiring and finance to external communications and event coordination.' /* notes */
    )
    ,(
        'https://www.openphilanthropy.org/blog/reflections-our-2018-generalist-research-analyst-recruiting', /* url */
        'Interview about CHAI', /* title */
        '2019-03-07', /* publication_date */
        NULL, /* modified_date */
        'Rosie Campbell', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'Rosie Campbell provides an accessible overview of the Center for Human-Compatible AI’s (CHAI) approach to ensuring AI technologies are aligned with human safety and well-being in a broad-audience interview with Mia Dand.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/iBTon2dRYwcoS9Jyr/predict-responses-to-the-existential-risk-from-ai-surveyy', /* url */
        'Predict responses to the "existential risk from AI" survey', /* title */
        '2018-03-26', /* publication_date */
        NULL, /* modified_date */
        ' Rob Bensinger', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'Rob Bensinger''s article provides an overview of a survey on existential risks from AI, examining the likelihood of future value losses due to insufficient technical AI safety research and system misalignment, with contributions from major AI organizations, including the Center for Human-Compatible AI.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/MskKEsj8nWREoMjQK/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1', /* url */
        'Introduction to Pragmatic AI Safety [Pragmatic AI Safety #1]', /* title */
        '2022-05-19', /* publication_date */
        NULL, /* modified_date */
        'TW123|Dan H', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article introduces "Pragmatic AI Safety" as an approach focusing on practical methods to address AI risks through collaboration with ML research, minimal capability enhancements, and sociotechnical insights, with contributions from the Center for Human-Compatible AI, aiming to improve alignment and risk mitigation efforts..' /* notes */
    )
    ,(
        'https://thegradientpub.substack.com/p/stuart-russell-the-foundations-of#details', /* url */
        'Stuart Russell: The Foundations of Artificial Intelligence', /* title */
        '2019-10-06', /* publication_date */
        NULL, /* modified_date */
        'Daniel Bashir', /* author */
        'The Gradient Pub', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'In this podcast episode, Stuart Russell discusses foundational AI concepts, focusing on the Center for Human-Compatible AI’s efforts to address control challenges in developing safe, rational AI systems.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/9WxdtLEfEDJfBAruX/are-we-actually-improving-decision-making', /* url */
        'Are we actually improving decision-making?', /* title */
        '2021-02-05', /* publication_date */
        NULL, /* modified_date */
        'Remmelt', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article critiques current field-building initiatives within effective altruism, emphasizing the need for more collaboration between EA groups and diverse professional stakeholders, such as those at the Center for Human-Compatible AI, to enhance balanced decision-making and avoid potential blindspots.' /* notes */
    )
    ,(
        'https://www.vox.com/the-highlight/23447596/artificial-intelligence-agi-openai-gpt3-existential-risk-human-extinction', /* url */
        'AI experts are increasingly afraid of what they’re creating', /* title */
        '2022-11-28', /* publication_date */
        NULL, /* modified_date */
        'Kelsey Piper', /* author */
        'Vox', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article highlights the Center for Human-Compatible AI''s role in researching and mitigating the existential risks posed by advanced artificial intelligence, with an emphasis on balancing innovation and human safety.' /* notes */
    )
    ,(
        'https://humancompatible.ai/news/2024/08/07/social-choice-should-guide-ai-alignment-in-dealing-with-diverse-human-feedback/#social-choice-should-guide-ai-alignment-in-dealing-with-diverse-human-feedback', /* url */
        'Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback', /* title */
        '2024-08-07', /* publication_date */
        NULL, /* modified_date */
        'Rachel Feedman', /* author */
        'Human Compatible AI/', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'Feedman and Holliday''s paper, under the Center for Human-Compatible AI, explores how social choice theory can guide the alignment of foundation models with collective human preferences, addressing challenges in aggregating divergent human inputs to enhance AI ethics and safety.' /* notes */
    )
    
;

